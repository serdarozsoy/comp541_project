{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.6",
      "display_name": "Julia 1.6"
    },
    "accelerator": "GPU",
    "colab": {
      "name": "SimCLRjl_pretraining_raw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serdarozsoy/comp541_project/blob/main/SimCLRjl_pretraining_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7S9cpFJqfXy"
      },
      "source": [
        "## Julia on Colaboratory ##\n",
        "\n",
        "[Colaboratory](https://colab.research.google.com) does not provide native support for the [Julia programming language](https://julialang.org). However, since Colaboratory gives you root access to the machine that runs your notebook (the *“runtime”* in Colaboratory terminology), we can install Julia support by uploading a specially crafted Julia notebook  – *this* notebook. We then install Julia and [IJulia](https://github.com/JuliaLang/IJulia.jl) ([Jupyter](https://jupyter.org)/Colaboratory notebook support) and reload the notebook so that Colaboratory detects and initiates what we installed.\n",
        "\n",
        "In brief:\n",
        "\n",
        "1. **Run the cell below**\n",
        "2. **Reload the page**\n",
        "3. **Edit the notebook name and start hacking Julia code below**\n",
        "\n",
        "**If your runtime resets**, either manually or if left idle for some time, **repeat steps 1 and 2**.\n",
        "\n",
        "### Acknowledgements ###\n",
        "\n",
        "This hack by Pontus Stenetorp is an adaptation of [James Bradbury’s original Colaboratory Julia hack](https://discourse.julialang.org/t/julia-on-google-colab-free-gpu-accelerated-shareable-notebooks/15319/27), that broke some time in September 2019 as Colaboratory increased their level of notebook runtime isolation. There also appears to be CUDA compilation support installed by default for each notebook runtime type in October 2019, which shaves off a good 15 minutes or so from the original hack’s installation time. Adapted for Knet by Deniz Yuret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrHjOFFsxf7W",
        "outputId": "d14bb2bf-666d-4b76-f4f0-0e627d23e976",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form"
      },
      "source": [
        "# @title Julia/IJulia installation cell\n",
        "# @markdown run once (takes 30 seconds) and reload page before running the next cell\n",
        "\n",
        "%%shell\n",
        "wget 'https://julialang-s3.julialang.org/bin/linux/x64/1.6/julia-1.6.3-linux-x86_64.tar.gz' -O /tmp/julia.tar.gz\n",
        "tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "rm /tmp/julia.tar.gz\n",
        "julia -e 'using Pkg; Pkg.add(\"IJulia\")'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-21 18:31:59--  https://julialang-s3.julialang.org/bin/linux/x64/1.6/julia-1.6.3-linux-x86_64.tar.gz\n",
            "Resolving julialang-s3.julialang.org (julialang-s3.julialang.org)... 151.101.2.49, 151.101.66.49, 151.101.130.49, ...\n",
            "Connecting to julialang-s3.julialang.org (julialang-s3.julialang.org)|151.101.2.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 gce internal redirect trigger\n",
            "Location: https://storage.googleapis.com/julialang2/bin/linux/x64/1.6/julia-1.6.3-linux-x86_64.tar.gz [following]\n",
            "--2022-01-21 18:31:59--  https://storage.googleapis.com/julialang2/bin/linux/x64/1.6/julia-1.6.3-linux-x86_64.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.203.128, 74.125.20.128, 74.125.135.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.203.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113399344 (108M) [application/x-tar]\n",
            "Saving to: ‘/tmp/julia.tar.gz’\n",
            "\n",
            "/tmp/julia.tar.gz   100%[===================>] 108.15M   289MB/s    in 0.4s    \n",
            "\n",
            "2022-01-21 18:31:59 (289 MB/s) - ‘/tmp/julia.tar.gz’ saved [113399344/113399344]\n",
            "\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
            "\u001b[?25h\u001b[2K\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[?25l\u001b[?25h\u001b[2K\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n",
            "\u001b[?25l\u001b[?25h\u001b[2K\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗ RELOAD NOTEBOOK before running below cell"
      ],
      "metadata": {
        "id": "eiFlh_Qc4dMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add required packages:"
      ],
      "metadata": {
        "id": "d3pGfulm9uew"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkBZp_6YP4gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4253ba-e4a0-4dc6-d760-ec370d0fbffa",
        "collapsed": true
      },
      "source": [
        "# Knet installation cell: run after reloading/renaming notebook\n",
        "# ENV[\"JULIA_CUDA_USE_BINARYBUILDER\"]=\"false\"  # Use this for faster installation, otherwise CUDA libraries will be downloaded\n",
        "using Pkg; Pkg.add(\"Knet\")\n",
        "using Pkg; Pkg.add([\"MLDatasets\",\"PyCall\",\"LinearAlgebra\",\"JLD2\"])\n",
        "import Pkg; Pkg.add(\"CUDA\")\n",
        "# to download resnet file\n",
        "import Pkg; Pkg.add(\"Git\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.6/Project.toml`\n",
            " \u001b[90m [033835bb] \u001b[39m\u001b[92m+ JLD2 v0.4.17\u001b[39m\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.6/Manifest.toml`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download ResNet.jl from SimCLR.jl:"
      ],
      "metadata": {
        "id": "sGewOei1-EEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using Git\n",
        "run(git([\"clone\", \"https://github.com/serdarozsoy/SimCLR.jl.git\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvM5WiHNXbFz",
        "outputId": "af53aaa6-8f22-4b82-ee9c-da3b696f946c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'SimCLR.jl'...\n",
            "/root/.julia/artifacts/33c5e3a13ad6427f86436f577c0edce1e468ac80/libexec/git-core/git-remote-https: /usr/local/bin/../lib/julia/libcurl.so.4: no version information available (required by /root/.julia/artifacts/33c5e3a13ad6427f86436f577c0edce1e468ac80/libexec/git-core/git-remote-https)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Process(setenv(`\u001b[4m/root/.julia/artifacts/33c5e3a13ad6427f86436f577c0edce1e468ac80/bin/git\u001b[24m \u001b[4mclone\u001b[24m \u001b[4mhttps://github.com/serdarozsoy/SimCLR.jl.git\u001b[24m`,[\"TF_FORCE_GPU_ALLOW_GROWTH=true\", \"CUDA_VERSION=11.1.1\", \"PATH=/root/.julia/artifacts/2a0fba617ae96fd7bec0e5f0981a2cc395998f08/bin:/root/.julia/artifacts/3831f541c8f90f52324b6020312a0cec7a8224aa/bin:/root/.julia/artifacts/75b2da9ed2ec48588460be0f3a8bb489212012e9/bin:/root/.julia/artifacts/33c5e3a13ad6427f86436f577c0edce1e468ac80/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin\", \"ENV=/root/.bashrc\", \"CLOUDSDK_CONFIG=/content/.config\", \"PWD=/\", \"GLIBCPP_FORCE_NEW=1\", \"__EGL_VENDOR_LIBRARY_DIRS=/usr/lib64-nvidia:/usr/share/glvnd/egl_vendor.d/\", \"DATALAB_SETTINGS_OVERRIDES={\\\"kernelManagerProxyPort\\\":6000,\\\"kernelManagerProxyHost\\\":\\\"172.28.0.3\\\",\\\"jupyterArgs\\\":[\\\"--ip\", \"COLUMNS=80\"  …  \"COLAB_GPU=1\", \"GIT_TEMPLATE_DIR=/root/.julia/artifacts/33c5e3a13ad6427f86436f577c0edce1e468ac80/share/git-core/templates\", \"LINES=30\", \"GIT_SSL_CAINFO=/usr/local/share/julia/cert.pem\", \"HOME=/root\", \"GCE_METADATA_TIMEOUT=0\", \"OPENBLAS_MAIN_FREE=1\", \"TBE_EPHEM_CREDS_ADDR=172.28.0.1:8009\", \"LIBRARY_PATH=/usr/local/cuda/lib64/stubs\", \"HOSTNAME=18dd2f478570\"]), ProcessExited(0))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretraining raw program to play at once:"
      ],
      "metadata": {
        "id": "PB6S8K8O-bE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using CUDA\n",
        "using Knet\n",
        "using MLDatasets\n",
        "using PyCall\n",
        "include(\"SimCLR.jl/resnet.jl\")\n",
        "using LinearAlgebra\n",
        "using JLD2\n",
        "\n",
        "\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "function load_cifar_dataset()\n",
        "    xtrn, ytrn = CIFAR10.traindata(UInt8);\n",
        "    # WxHxCxB -> HxWxCxB \n",
        "    xtrn = permutedims(xtrn, (2, 1, 3, 4))\n",
        "    xtrn, ytrn\n",
        "end\n",
        "\n",
        "\n",
        "# Pycall implementation to use torchvision library for data augmentation \n",
        "py\"\"\"\n",
        "from PIL import Image, ImageFilter, ImageOps\n",
        "from torchvision import transforms\n",
        "\n",
        "def init():\n",
        "    return transforms.Compose(\n",
        "                    [   transforms.ToPILImage(),\n",
        "                        transforms.RandomResizedCrop(\n",
        "                            (32, 32),\n",
        "                            scale=(0.08, 1.0) \n",
        "                        ),\n",
        "                        transforms.RandomApply(\n",
        "                            [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8\n",
        "                        ),\n",
        "                        transforms.RandomGrayscale(p=0.2),\n",
        "                        transforms.RandomHorizontalFlip(p=0.5),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "\n",
        "def transform_x(transform_model, x):\n",
        "    return transform_model(x)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "function transform(x)\n",
        "    return py\"transform_x\"(py\"init\"(), x)[:numpy]()\n",
        "end\n",
        "\n",
        "\n",
        "# load dataset\n",
        "X_train, y_train = load_cifar_dataset();\n",
        "\n",
        "\n",
        "# Model: ResNet + Projection Head\n",
        "model_main = Sequential(ResNet(depth=18, in_ch=3, out_ch=512, cifar_stem=true), Linear(512,512,f=relu), Linear(512,128))\n",
        "\n",
        "\n",
        "# Parameters\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Define optimizer\n",
        "for p in params(model_main)\n",
        "    p.opt = Adam(lr=learning_rate)\n",
        "end\n",
        "\n",
        "# Contrastive loss\n",
        "function loss_contrastive(model, x, bs)\n",
        "\n",
        "    LARGE_NUM = 1e9\n",
        "    temperature = 0.5\n",
        "    \n",
        "    z = transpose(model(x))\n",
        "    \n",
        "    _atype = KnetArray{Float32}    \n",
        "    norm_z = sqrt.(sum(abs2,z,dims=2))\n",
        "\n",
        "    zx = z ./ norm_z\n",
        "\n",
        "    z1 = zx[1:bs,:]\n",
        "    z2 = zx[bs+1:bs*2,:]\n",
        "    \n",
        "    \n",
        "    labels = convert(_atype, Array{Float64}(I, bs, bs*2))\n",
        "    mask = convert(_atype, Array{Float64}(I, bs, bs)*LARGE_NUM)\n",
        "    \n",
        "    logits_aa = z1*transpose(z1)/temperature - mask \n",
        "    logits_bb = z2*transpose(z2)/temperature - mask \n",
        "    logits_ab = z1*transpose(z2)/temperature\n",
        "    logits_ba = z2*transpose(z1)/temperature\n",
        "    \n",
        "    loss_a = sum(-labels.*logsoftmax([logits_ab logits_aa], dims=2))/bs\n",
        "    loss_b = sum(-labels.*logsoftmax([logits_ba logits_bb], dims=2))/bs\n",
        "    \n",
        "    loss = loss_a + loss_b\n",
        "\n",
        "    return loss\n",
        "end\n",
        "\n",
        "\n",
        "train_loss = zeros(epochs);\n",
        "_atype = KnetArray{Float32}\n",
        "\n",
        "# Training\n",
        "for epoch in 1:epochs\n",
        "    train_loss[epoch] = 0.0\n",
        "    batch_count = 0\n",
        "    dtrn = minibatch(X_train, y_train, 256, shuffle=true)\n",
        "    for (batch_x, batch_y) in dtrn\n",
        "        bs = size(batch_x)[4]\n",
        "        batch_x1 = cat([transform(batch_x[:,:,:,i]) for i in 1:bs]..., dims=4)\n",
        "        batch_x2 = cat([transform(batch_x[:,:,:,i]) for i in 1:bs]..., dims=4) \n",
        "        x2 = convert(_atype, permutedims(cat(batch_x1 , batch_x2, dims=4), (2, 3, 1, 4)))\n",
        "        loss_main= @diff loss_contrastive(model_main, x2, bs)\n",
        "        train_loss[epoch] += value(loss_main)\n",
        "        batch_count += 1\n",
        "        for p in params(model_main)\n",
        "            g = grad(loss_main, p)\n",
        "            update!(value(p), g, p.opt)\n",
        "        end\n",
        "    end\n",
        "    train_loss[epoch] /= batch_count\n",
        "    println(\"Train epoch: \", epoch ,\" ,loss: \", train_loss[epoch])\n",
        "    flush(stdout)\n",
        "    if mod(epoch, 100) == 0\n",
        "      model_name = string(\"results/model_main_\", epoch, \".jld2\") \n",
        "      save(model_name, \"model\", model_main)\n",
        "    end\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD4oNHiNuETa",
        "outputId": "4600cd6b-38df-4aac-8792-1d895dd55961"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This program has requested access to the data dependency CIFAR10.\n",
            "which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
            "\n",
            "Dataset: The CIFAR-10 dataset\n",
            "Authors: Alex Krizhevsky, Vinod Nair, Geoffrey Hinton\n",
            "Website: https://www.cs.toronto.edu/~kriz/cifar.html\n",
            "Reference: https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf\n",
            "\n",
            "[Krizhevsky, 2009]\n",
            "    Alex Krizhevsky.\n",
            "    \"Learning Multiple Layers of Features from Tiny Images\",\n",
            "    Tech Report, 2009.\n",
            "\n",
            "The CIFAR-10 dataset is a labeled subsets of the 80\n",
            "million tiny images dataset. It consists of 60000\n",
            "32x32 colour images in 10 classes, with 6000 images\n",
            "per class.\n",
            "\n",
            "The compressed archive file that contains the\n",
            "complete dataset is available for download at the\n",
            "offical website linked above; specifically the binary\n",
            "version for C programs. Note that using the data\n",
            "responsibly and respecting copyright remains your\n",
            "responsibility. The authors of CIFAR-10 aren't really\n",
            "explicit about any terms of use, so please read the\n",
            "website to make sure you want to download the\n",
            "dataset.\n",
            "\n",
            "\n",
            "\n",
            "Do you want to download the dataset from https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz to \"/root/.julia/datadeps/CIFAR10\"?\n",
            "[y/n]\n",
            "stdin> y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: CUDNN\n",
            "ERROR: Unexpected end of data : jl_WV3E7K-download\n",
            "\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: CUDNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train epoch: 1 ,loss: 10.947704990093525\n",
            "Train epoch: 2 ,loss: 10.429761637174167\n",
            "Train epoch: 3 ,loss: 10.219364239619328\n",
            "Train epoch: 4 ,loss: 10.113775869516227\n",
            "Train epoch: 5 ,loss: 10.029555403880584\n",
            "Train epoch: 6 ,loss: 9.966238100100787\n",
            "Train epoch: 7 ,loss: 9.907828815166766\n",
            "Train epoch: 8 ,loss: 9.871267338288137\n",
            "Train epoch: 9 ,loss: 9.834329849634415\n",
            "Train epoch: 10 ,loss: 9.80773967351669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x68u-_youbCr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}